
#
# =================================================================================================
# Notebook: 05_unsupervised_learning.ipynb
#
# Description:
# This notebook applies unsupervised learning (K-Means and Hierarchical Clustering)
# to find patterns in the data without using labels.
# =================================================================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# Use the feature-selected dataset for clustering
try:
    df = pd.read_csv('feature_selected_dataset.csv')
    print("Feature-selected dataset loaded.")
except FileNotFoundError:
    print("Error: 'feature_selected_dataset.csv' not found.")
    exit()

X = df.drop('target', axis=1)
y_true = df['target'] # Keep true labels for comparison

# =================================================================================================
# Sprint 2.5: Unsupervised Learning - Clustering
# =================================================================================================

# -------------------------------------------------------------------------------------------------
# Step 1: Apply K-Means Clustering (use elbow method to determine K).
# -------------------------------------------------------------------------------------------------
print("\n--- Applying K-Means Clustering ---")
wcss = [] # Within-cluster sum of squares
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plot the elbow curve
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS')
plt.grid()
plt.show()

# The "elbow" appears at K=2 or K=3. Let's choose K=2 as we have 2 target classes.
optimal_k = 2
kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42, n_init=10)
y_kmeans = kmeans.fit_predict(X)

# -------------------------------------------------------------------------------------------------
# Step 2: Perform Hierarchical Clustering (dendrogram analysis).
# -------------------------------------------------------------------------------------------------
print("\n--- Performing Hierarchical Clustering ---")
linked = linkage(X, method='ward')

plt.figure(figsize=(15, 8))
dendrogram(linked,
           orientation='top',
           distance_sort='descending',
           show_leaf_counts=True)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

# Apply hierarchical clustering with 2 clusters
hc = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
y_hc = hc.fit_predict(X)

# -------------------------------------------------------------------------------------------------
# Step 3: Compare clusters with actual disease labels.
# -------------------------------------------------------------------------------------------------
print("\n--- Comparing Clusters with Actual Labels ---")
df['kmeans_cluster'] = y_kmeans
df['hc_cluster'] = y_hc

# Crosstab for K-Means
kmeans_crosstab = pd.crosstab(df['target'], df['kmeans_cluster'])
print("\nK-Means Clustering Crosstab:")
print(kmeans_crosstab)

# Crosstab for Hierarchical Clustering
hc_crosstab = pd.crosstab(df['target'], df['hc_cluster'])
print("\nHierarchical Clustering Crosstab:")
print(hc_crosstab)
