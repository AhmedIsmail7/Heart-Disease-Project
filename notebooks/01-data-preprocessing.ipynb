{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile notebooks/01_data_preprocessing.ipynb\n",
    "\n",
    "#\n",
    "# =================================================================================================\n",
    "# COMPREHENSIVE MACHINE LEARNING FULL PIPELINE ON HEART DISEASE UCI DATASET\n",
    "# =================================================================================================\n",
    "#\n",
    "# Notebook: 01_data_preprocessing.ipynb\n",
    "#\n",
    "# Description:\n",
    "# This notebook covers the initial phase of the project: Data Preprocessing and Cleaning. [cite: 29]\n",
    "# It includes loading the dataset, handling missing values, encoding categorical features,\n",
    "# scaling numerical features, and performing exploratory data analysis (EDA).\n",
    "#\n",
    "# =================================================================================================\n",
    "\n",
    "# Required Library Imports [cite: 20]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =================================================================================================\n",
    "# Sprint 2.1: Data Preprocessing & Cleaning [cite: 29]\n",
    "# =================================================================================================\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Step 1: Load the Heart Disease UCI dataset into a Pandas DataFrame. [cite: 31]\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "print(\"Step 1: Loading the dataset...\")\n",
    "# The dataset is often available in UCI's repository. We'll use a common public URL for it.\n",
    "# This assumes you've uploaded 'heart_disease.csv' to the '/kaggle/input/' directory if running on Kaggle.\n",
    "try:\n",
    "    # Attempt to load from a local Kaggle path first\n",
    "    df = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\n",
    "    print(\"Dataset loaded successfully from local path.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Local file not found. Loading from a remote URL...\")\n",
    "    # Fallback to a remote URL if local file isn't found\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\n",
    "    # The original dataset does not have column headers. We need to add them.\n",
    "    column_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
    "    df = pd.read_csv(url, header=None, names=column_names)\n",
    "    print(\"Dataset loaded successfully from remote URL.\")\n",
    "\n",
    "df.to_csv('data/heart_disease.csv', index=False)\n",
    "\n",
    "print(\"\\nInitial Dataset Info:\")\n",
    "df.info()\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Step 2: Handle missing values (imputation or removal). [cite: 32]\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "print(\"\\nStep 2: Handling missing values...\")\n",
    "# The original dataset from UCI represents missing values with '?'.\n",
    "# We need to replace '?' with NaN (Not a Number) to handle them properly.\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "print(\"\\nMissing values before handling:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 'ca' (number of major vessels) and 'thal' are object types due to '?'. Let's convert them to numeric.\n",
    "df['ca'] = pd.to_numeric(df['ca'])\n",
    "df['thal'] = pd.to_numeric(df['thal'])\n",
    "\n",
    "# Impute missing values. For simplicity, we'll use the median for numeric columns.\n",
    "# Median is often better for skewed distributions, which is common in medical data.\n",
    "for col in ['ca', 'thal']:\n",
    "    median_val = df[col].median()\n",
    "    df[col].fillna(median_val, inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Step 3: Perform data encoding (one-hot encoding for categorical variables). [cite: 33]\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "print(\"\\nStep 3: Performing one-hot encoding for categorical variables...\")\n",
    "# Identify categorical columns for one-hot encoding.\n",
    "# While some of these are numeric (like 'sex', 'cp'), they represent categories.\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(\"\\nDataset shape after one-hot encoding:\", df.shape)\n",
    "print(\"\\nFirst 5 rows of the dataset after encoding:\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Step 4: Standardize numerical features using StandardScaler. [cite: 34]\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "print(\"\\nStep 4: Standardizing numerical features...\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify numerical columns for scaling\n",
    "numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "print(\"\\nFirst 5 rows of the dataset after scaling:\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Step 5: Conduct Exploratory Data Analysis (EDA). [cite: 35]\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "print(\"\\nStep 5: Conducting Exploratory Data Analysis (EDA)...\")\n",
    "\n",
    "# --- Correlation Heatmap ---\n",
    "plt.figure(figsize=(18, 15))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Correlation Heatmap of Features', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# --- Histograms for Numerical Features ---\n",
    "print(\"\\nDisplaying histograms for original numerical features (before scaling)...\")\n",
    "# For more interpretable histograms, we'll reload the data before scaling and encoding\n",
    "try:\n",
    "    df_eda = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\n",
    "except FileNotFoundError:\n",
    "    df_eda = pd.read_csv(url, header=None, names=column_names)\n",
    "df_eda.replace('?', np.nan, inplace=True)\n",
    "df_eda.dropna(inplace=True) # Drop NA for simplicity in EDA plots\n",
    "for col in ['ca', 'thal']:\n",
    "    df_eda[col] = pd.to_numeric(df_eda[col])\n",
    "\n",
    "df_eda[numerical_cols].hist(bins=20, figsize=(15, 10), layout=(2, 3))\n",
    "plt.suptitle('Histograms of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "# --- Boxplots for Numerical Features vs. Target ---\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.boxplot(x='target', y=col, data=df_eda)\n",
    "    plt.title(f'{col} vs. Target')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEDA visualizations have been generated.\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Convert to Binary Classification Target\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "print(\"\\nConverting target to binary classification (0: No Disease, 1: Disease)...\")\n",
    "# Any value > 0 in the 'target' column becomes 1, otherwise it stays 0.\n",
    "df['target'] = (df['target'] > 0).astype(int)\n",
    "\n",
    "print(\"\\nValue counts of the new binary target:\")\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "# Check the distribution of the binary target\n",
    "distribution = df['target'].value_counts(normalize=True) * 100\n",
    "print(\"Target variable distribution:\")\n",
    "print(distribution)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Deliverable: Cleaned dataset ready for modeling\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "print(\"\\nData preprocessing and cleaning complete.\")\n",
    "# Saving the cleaned dataset with the new binary target\n",
    "df.to_csv('cleaned_heart_disease.csv', index=False)\n",
    "print(\"Cleaned dataset saved to 'cleaned_heart_disease.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
