{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile notebooks/01_data_preprocessing.ipynb\n\n#\n# =================================================================================================\n# COMPREHENSIVE MACHINE LEARNING FULL PIPELINE ON HEART DISEASE UCI DATASET\n# =================================================================================================\n#\n# Notebook: 01_data_preprocessing.ipynb\n#\n# Description:\n# This notebook covers the initial phase of the project: Data Preprocessing and Cleaning. [cite: 29]\n# It includes loading the dataset, handling missing values, encoding categorical features,\n# scaling numerical features, and performing exploratory data analysis (EDA).\n#\n# =================================================================================================\n\n# Required Library Imports [cite: 20]\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# =================================================================================================\n# Sprint 2.1: Data Preprocessing & Cleaning [cite: 29]\n# =================================================================================================\n\n# -------------------------------------------------------------------------------------------------\n# Step 1: Load the Heart Disease UCI dataset into a Pandas DataFrame. [cite: 31]\n# -------------------------------------------------------------------------------------------------\nprint(\"Step 1: Loading the dataset...\")\n# The dataset is often available in UCI's repository. We'll use a common public URL for it.\n# This assumes you've uploaded 'heart_disease.csv' to the '/kaggle/input/' directory if running on Kaggle.\ntry:\n    # Attempt to load from a local Kaggle path first\n    df = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\n    print(\"Dataset loaded successfully from local path.\")\nexcept FileNotFoundError:\n    print(\"Local file not found. Loading from a remote URL...\")\n    # Fallback to a remote URL if local file isn't found\n    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\n    # The original dataset does not have column headers. We need to add them.\n    column_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n    df = pd.read_csv(url, header=None, names=column_names)\n    print(\"Dataset loaded successfully from remote URL.\")\n\ndf.to_csv('data/heart_disease.csv', index=False)\n\nprint(\"\\nInitial Dataset Info:\")\ndf.info()\nprint(\"\\nFirst 5 rows of the dataset:\")\nprint(df.head())\n\n\n# -------------------------------------------------------------------------------------------------\n# Step 2: Handle missing values (imputation or removal). [cite: 32]\n# -------------------------------------------------------------------------------------------------\nprint(\"\\nStep 2: Handling missing values...\")\n# The original dataset from UCI represents missing values with '?'.\n# We need to replace '?' with NaN (Not a Number) to handle them properly.\ndf.replace('?', np.nan, inplace=True)\n\nprint(\"\\nMissing values before handling:\")\nprint(df.isnull().sum())\n\n# 'ca' (number of major vessels) and 'thal' are object types due to '?'. Let's convert them to numeric.\ndf['ca'] = pd.to_numeric(df['ca'])\ndf['thal'] = pd.to_numeric(df['thal'])\n\n# Impute missing values. For simplicity, we'll use the median for numeric columns.\n# Median is often better for skewed distributions, which is common in medical data.\nfor col in ['ca', 'thal']:\n    median_val = df[col].median()\n    df[col].fillna(median_val, inplace=True)\n\nprint(\"\\nMissing values after handling:\")\nprint(df.isnull().sum())\n\n\n# -------------------------------------------------------------------------------------------------\n# Step 3: Perform data encoding (one-hot encoding for categorical variables). [cite: 33]\n# -------------------------------------------------------------------------------------------------\nprint(\"\\nStep 3: Performing one-hot encoding for categorical variables...\")\n# Identify categorical columns for one-hot encoding.\n# While some of these are numeric (like 'sex', 'cp'), they represent categories.\ncategorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\ndf = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n\nprint(\"\\nDataset shape after one-hot encoding:\", df.shape)\nprint(\"\\nFirst 5 rows of the dataset after encoding:\")\nprint(df.head())\n\n\n# -------------------------------------------------------------------------------------------------\n# Step 4: Standardize numerical features using StandardScaler. [cite: 34]\n# -------------------------------------------------------------------------------------------------\nprint(\"\\nStep 4: Standardizing numerical features...\")\nfrom sklearn.preprocessing import StandardScaler\n\n# Identify numerical columns for scaling\nnumerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n\nscaler = StandardScaler()\ndf[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n\nprint(\"\\nFirst 5 rows of the dataset after scaling:\")\nprint(df.head())\n\n\n# -------------------------------------------------------------------------------------------------\n# Step 5: Conduct Exploratory Data Analysis (EDA). [cite: 35]\n# -------------------------------------------------------------------------------------------------\nprint(\"\\nStep 5: Conducting Exploratory Data Analysis (EDA)...\")\n\n# --- Correlation Heatmap ---\nplt.figure(figsize=(18, 15))\nsns.heatmap(df.corr(), annot=True, cmap='viridis', fmt='.2f')\nplt.title('Correlation Heatmap of Features', fontsize=20)\nplt.show()\n\n# --- Histograms for Numerical Features ---\nprint(\"\\nDisplaying histograms for original numerical features (before scaling)...\")\n# For more interpretable histograms, we'll reload the data before scaling and encoding\ntry:\n    df_eda = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\nexcept FileNotFoundError:\n    df_eda = pd.read_csv(url, header=None, names=column_names)\ndf_eda.replace('?', np.nan, inplace=True)\ndf_eda.dropna(inplace=True) # Drop NA for simplicity in EDA plots\nfor col in ['ca', 'thal']:\n    df_eda[col] = pd.to_numeric(df_eda[col])\n\ndf_eda[numerical_cols].hist(bins=20, figsize=(15, 10), layout=(2, 3))\nplt.suptitle('Histograms of Numerical Features')\nplt.show()\n\n# --- Boxplots for Numerical Features vs. Target ---\nplt.figure(figsize=(15, 10))\nfor i, col in enumerate(numerical_cols):\n    plt.subplot(2, 3, i + 1)\n    sns.boxplot(x='target', y=col, data=df_eda)\n    plt.title(f'{col} vs. Target')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nEDA visualizations have been generated.\")\n\n\n# -------------------------------------------------------------------------------------------------\n# Convert to Binary Classification Target\n# -------------------------------------------------------------------------------------------------\nprint(\"\\nConverting target to binary classification (0: No Disease, 1: Disease)...\")\n# Any value > 0 in the 'target' column becomes 1, otherwise it stays 0.\ndf['target'] = (df['target'] > 0).astype(int)\n\nprint(\"\\nValue counts of the new binary target:\")\nprint(df['target'].value_counts())\n\n# Check the distribution of the binary target\ndistribution = df['target'].value_counts(normalize=True) * 100\nprint(\"Target variable distribution:\")\nprint(distribution)\n\n# -------------------------------------------------------------------------------------------------\n# Deliverable: Cleaned dataset ready for modeling\n# -------------------------------------------------------------------------------------------------\nprint(\"\\nData preprocessing and cleaning complete.\")\n# Saving the cleaned dataset with the new binary target\ndf.to_csv('cleaned_heart_disease.csv', index=False)\nprint(\"Cleaned dataset saved to 'cleaned_heart_disease.csv'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}